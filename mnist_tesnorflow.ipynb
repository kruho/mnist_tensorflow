{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_train, nb_val = 50000, 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "f = np.load('mnist.npz')\n",
    "X = f['x_train'] / 255.\n",
    "Y = f['y_train']\n",
    "x_train = X[:nb_train]\n",
    "y_train = Y[:nb_train]\n",
    "x_val = X[nb_train:nb_train+nb_val]\n",
    "y_val = Y[nb_train:nb_train+nb_val]\n",
    "x_test = f['x_test'] / 255.\n",
    "y_test = f['y_test']\n",
    "\n",
    "def batch_flatten(X):\n",
    "    return np.reshape(X, (len(X), np.prod(X.shape[1:])))\n",
    "    \n",
    "x_train = batch_flatten(x_train)\n",
    "x_val   = batch_flatten(x_val)\n",
    "x_test  = batch_flatten(x_test)\n",
    "\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test  = np.eye(10)[y_test]\n",
    "y_val   = np.eye(10)[y_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ミニバッチ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(x, y, nb_samples):\n",
    "    a = 0\n",
    "    while True:\n",
    "        yield x[a:a+nb_samples], y[a:a+nb_samples]\n",
    "        a += nb_samples\n",
    "        if a+nb_samples>len(x):\n",
    "            x,y = shuffle(x,y)\n",
    "            a = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxA+hxA+hxA+hxA+hxA+hxA+hxA+h/gdqY2RigYrulQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f40540650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_gen = next_batch(x_train, y_train, 100)\n",
    "x_batch, y_batch = batch_gen.next()\n",
    "\n",
    "plt.imshow(np.reshape(x_batch[0],(28,28)), 'gray')\n",
    "plt.title(str(y_batch[0].argmax()))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x_input = tf.placeholder(tf.float32, [None, 784], name='x_input')\n",
    "\n",
    "if False:\n",
    "    h = tf.layers.dense(x_input, 500, activation=tf.nn.relu)\n",
    "    h = tf.layers.batch_normalization(h)\n",
    "    h = tf.layers.dense(h, 10, activation=None)\n",
    "    h = tf.layers.batch_normalization(h)\n",
    "    y_pred = tf.nn.softmax(h, name='y_pred')\n",
    "elif True:\n",
    "    W1 = tf.Variable(tf.random_normal([784, 500], stddev=np.sqrt(2 / (784 + 500))), name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([500]), name='b1')\n",
    "    W2 = tf.Variable(tf.random_normal([500, 10], stddev=np.sqrt(2 / (500 + 10))), name='W2')\n",
    "    b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "    W3 = tf.Variable(tf.random_normal([784, 10], stddev=np.sqrt(2 / (784 + 10))), name='W3')\n",
    "    b3 = tf.Variable(tf.zeros([10]), name='b3')\n",
    "#    h = tf.nn.relu(tf.add(tf.matmul(x_input, W1), b1), name='h')\n",
    "#    y_pred = tf.nn.softmax(tf.add(tf.matmul(h, W2), b2), name='y_pred')\n",
    "    h = tf.nn.elu(tf.nn.bias_add(tf.matmul(x_input, W1), b1), name='h')\n",
    "#    h = tf.nn.bias_add(tf.matmul(x_input, W1), b1)\n",
    "    y_pred = tf.nn.softmax(tf.nn.bias_add(tf.matmul(h, W2), b2), name='y_pred')\n",
    "#    y_pred = tf.nn.softmax(tf.nn.bias_add(tf.matmul(x_input, W3), b3), name='y_pred')\n",
    "else:\n",
    "    W = tf.Variable(tf.zeros([784, 10]), name='W')\n",
    "    b = tf.Variable(tf.zeros([10]), name='b')\n",
    "    y_pred = tf.nn.softmax(tf.matmul(x_input, W) + b, name='y_pred')\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, [None, 10], name='y_true')\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_pred), reduction_indices=[1]))\n",
    "#cross_entropy = tf.reduce_sum(- y_true * tf.log(y_pred), 1)\n",
    "#cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred, name='xentropy')\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_true, axis=1), tf.argmax(y_pred, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "#    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    # Op to calculate every variable gradient\n",
    "    grads = tf.gradients(cross_entropy, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    # Op to update all variables according to their gradient\n",
    "    apply_grads = optimizer.apply_gradients(grads_and_vars=grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:16<00:00, 59.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "if True:\n",
    "    tf.summary.histogram(\"logs/weights1\",W1)\n",
    "    tf.summary.histogram(\"logs/weights2\",W2)\n",
    "    tf.summary.histogram(\"logs/bias1\",b1)\n",
    "    tf.summary.histogram(\"logs/bias2\",b2)\n",
    "elif False:\n",
    "    tf.summary.histogram(\"logs/hidden\",h)\n",
    "#tf.summary.histogram(\"logs/weights3\",W3)\n",
    "#tf.summary.histogram(\"logs/bias3\",b3)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "logdir = \"tf_logs/.../\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "summary_writer = tf.summary.FileWriter(logdir, graph_def=sess.graph_def)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    batch_xs, batch_ys = batch_gen.next()\n",
    "    _, c, summary = sess.run([apply_grads, cross_entropy, merged_summary_op], feed_dict={x_input: batch_xs.astype(np.float32), y_true: batch_ys.astype(np.float32)})\n",
    "    summary_writer.add_summary(summary, i)\n",
    "#    if not i%1000:\n",
    "#        print(sess.run(accuracy, feed_dict={x_input: x_val, y_true: y_val}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1938\n",
      "0.1938\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.eval({x_input: x_test, y_true: y_test}))\n",
    "print(sess.run(accuracy, feed_dict={x_input: x_test, y_true: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
